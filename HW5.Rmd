---
title: 'HW5: Team 12'
author: 'Thomas Fleming, Blaire Li, Marc D. Ryser, Hengqian Zhang'
date: "Due March 10, 2016"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---


```{r setup, warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE)
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(GGally))
library(BAS)
library(knitr)
# post on piazza for additional packages if there are wercker build errors due to missing packages
```

For this assignment we will explore simulation of data to compare methods for estimation and model selection.  To get started, refer to the code from Lab6 and simulate the datasets as described there.  Some "guideposts" for when to finish parts are provided within the problem set.

1.  Add to the Lab6 code a second set of 100 datasets for testing (prediction) with $25$ observations, but where the $X$'s have the same correlation matrix as in the training data.   Provide a brief description of the model that generated the data and summary of the simulation study.  (ie dimensions, true $\beta$ etc, number of simulated datasets etc.). _(Finish Monday: most code is from lab; modification fo add test data should be straightforward)_


Answer:
The true betas' we choose is the same as the betas' in lab6 which are {4.2,0,0,0,-1,0,1.5,0,0,0,1,0,0.5,0,0,0,0,-1,1,4}. We simulated 100 datasets and each with dimention 25 $\times$ 21. The procedue we use to generate the data is exact the same as we did in lab 6. Considering the training data set, we wanted to simulate 100 datasets each with dimention 75 $\times$ 23(plus Y and mu)
For each dataset, first we generated 75 $\times$ 10 from standard normal distribution.

Secondly, we picked up the last 5 columns of the data we generated in step 1 and did matrix multiplication with {0.3,0.5,0.7,0.9,1.1} to get 75$\times$ 1 vector and  replicated it 5 times to get X1 with dimention 75 $\times$ 5

Thirdly, we generated 75 $\times$ 4 matrix from standard normal distribution and picked up the 4th column and added error from normal(0,0.1) to get X2 and X3 respectively.

Finally, we column binded X1, X2, X3 to get desired dataset and also computed Y and mu.

In addition, we can see the similar pattern in both correlation heatmap of training data and testing data. In simulation, there are 5 variables(columns) have correlation to each other. In heat map, there are ranctangles around the diagnal indicating the same patterns.
```{r}
#HOW DO WE MAKE THE TEST X's HAVE THE SAME CORRELATION MATRIX AS IN THE TRAINING DATA?

# true parameters
sigma = 2.5
betatrue = c(4,2,0,0,0,-1,0,1.5, 0,0,0,1,0,.5,0,0,0,0,-1,1,4)
#          int|    X1                            | X2     |X3 

names(betatrue) = c("(Intercept)","x1","x2","x3","x4","x5",
                       "x6","x7","x8","x9","x10",
                       "x11","x12","x13","x14","x15",
                       "x16","x17","x18","x19","x20")

#sample size
n = 75

# part of dataframe name
fname=rep("df",100)

# create 100 datasets
for (i in 1:100) {
  
# generate some standard normals
  Z = matrix(rnorm(n*10, 0, 1), ncol=10, nrow=n)
  
#  Create X1 by taking linear cominations of Z to induce correlation among X1 components
  #multiplying first 5 columns by vector of numbers to create linear dependence
  X1 = cbind(Z, (Z[,1:5] %*% c(.3, .5, .7, .9, 1.1) %*% t(rep(1,5)) +
             matrix(rnorm(n*5, 0, 1), ncol=5, nrow=n)))
# generate X2 as a standard normal  
  #X2 independent, but correlated with X3
  X2 <- matrix(rnorm(n*4,0,1), ncol=4, nrow=n)
  
# Generate X3 as a linear combination of X2 and noise  
  X3 <- X2[,4]+rnorm(n,0,sd=0.1)
  
# combine them  
  X <- cbind(X1,X2,X3)
  
# subtract off the column means
  X = sweep(X, 2, apply(X,2, mean)) 

# Generate mu     
# X does not have a column of ones for the intercept so need to add the intercept  
# for true mu  
  mu = betatrue[1] + X %*% betatrue[-1] 
  
# now generate Y  
  Y = mu + rnorm(n,0,sigma)  
  
# make a dataframe and save it
  df = data.frame(Y, X, mu)
  colnames(df) = c("Y","x1","x2","x3","x4","x5",
                       "x6","x7","x8","x9","x10",
                       "x11","x12","x13","x14","x15",
                       "x16","x17","x18","x19","x20",
                   "mu")
  fname[i] = paste("df", as.character(i), sep="")
  save(df, file=fname[i])
}

load(fname[1])

par(mfrow = c(1,2))
#correlation map for training data 
image(cor(df[1:50,2:22]))
#correlation map for testing data 
image(cor(df[51:75,2:22]))
```


2.  Using Ordinary Least squares based on fitting the full model for each of the 100 data sets,  Compute the average RMSE for a) estimating $\beta^{true}$, b) estimating
$\mu^{true} = X \beta^{true}$ and c) out of sample prediction for the test data from the 100 data sets. Present histograms of the RMSEs and show where the average falls.
Note for a vector of length $d$, RMSE is defined as
$$
RMSE(\hat{\theta}) = \sqrt{\sum_{i = 1}^{d} (\hat{\theta}_j - \theta_j)^2/d}
$$
_(Finish Monday as this code from lab can be directly used/modified for this)_

```{r}
#REVIEW; RSME for mu probably not computed correctly; verify out-of-sample RSME was computed correctly
rmse = function(y, ypred){
  rmse=sqrt(mean((y-ypred)^2))
  return(rmse)
}

RMSE.beta  = rep(0,100)
RMSE.mu  = rep(0,100)
RMSE.test.pred  = rep(0,100)
ls = vector("list",100)

for( i in 1:100) {
  rm(df)
  load(fname[i])
  
  ls[[i]]= lm(Y ~ . -mu, data=df[1:50,])
  
  coef.ols = coef(ls[[i]])
  ## for beta
  RMSE.beta[i] = rmse(betatrue,coef.ols)
  ## for mu
  RMSE.mu[i] = rmse(mu, ls[[i]]$fitted.values)
  ## for test data
  RMSE.test.pred[i] = rmse(Y,predict(ls[[i]], data = df[51:75,]))
#  print(c(i, MSE.OLS[i]))
  
}

#  average RMSE for estimating beta
avg.rmse.beta = mean(RMSE.beta)

#  average RMSE for estimating mu
avg.rmse.mu = mean(RMSE.mu)

# average RMSE for testing data
avg.rmse.test = mean(RMSE.test.pred)

#print average root mean squared errors
print(avg.rmse.beta)
print(avg.rmse.mu)
print(avg.rmse.test)

#store RSME's in data frame
rmse.df = data.frame(RMSE.beta, RMSE.mu, RMSE.test.pred)

#beta
ggplot(data = rmse.df[1], aes(x = rmse.df[1])) + geom_histogram(fill = "lightblue", color = "darkblue") + geom_vline(xintercept = avg.rmse.beta, color = "red")

#mu
ggplot(data = rmse.df[2], aes(x = rmse.df[2])) + geom_histogram(fill = "khaki", color = "darkgoldenrod") + geom_vline(xintercept = avg.rmse.mu, color = "darkblue")

#test prediction
ggplot(data = rmse.df[3], aes(x = rmse.df[3])) + geom_histogram(fill = "salmon", color = "orangered3") + geom_vline(xintercept = avg.rmse.test, color = "darkblue")
```


3.  Use AIC with either stepwise or all possible subsets to select a model and then use OLS to estimate the parameters under that model.  Using the estimates to compute the RMSE for a) estimating $\beta^{true}$, b) estimating $\mu^{true}$, and c) predicting $Y^{test}$. Present  histograms of the RMSE, and show where the  average RMSE falls on the plot.   Also report d) the number of times you select the true model using AIC out of the 100 simulations. _(A little more challenging: discuss with team  by Tuesday.  Figuring this out how to calculate RMSE with model selection will be needed for subsequent parts so start this early!.  Once this is done the  problem 5 should be easy!  Your write up should make it clear whether you used stepwise or all possible subsets)_ 

```{r}
#create vector with the same length and same name of betatrue
beta_like = numeric(length(betatrue))
names(beta_like) = names(betatrue)

#initialize RMSE vector
RMSE_beta_aic  = rep(0,100)
RMSE_mu_aic  = rep(0,100)
RMSE_test_aic  = rep(0,100)

for(i in 1:100){
#stepwise selection with AIC
app_best = step(ls[[i]], k=2,trace=0)
#create a copy of beta_like
beta_e = beta_like
#fill estimated beta with 0
beta_e[names(app_best$coefficients)] = app_best$coefficients
## for beta
RMSE_beta_aic[i] = rmse(betatrue,beta_e)
## for mu
RMSE_mu_aic[i] = rmse(mu, app_best$fitted.values)
## for testing data
RMSE_test_aic[i] = rmse(Y,predict(app_best, data = df[51:75,]))
}

#  average RMSE for estimating beta
avg_rmse_beta = mean(RMSE_beta_aic)

#  average RMSE for estimating mu
avg_rmse_mu = mean(RMSE_mu_aic)

# average RMSE for testing data
avg_rmse_test = mean(RMSE_test_aic)

#store RSME's in data frame
rmse_df_aic = data.frame(RMSE_beta_aic, RMSE_mu_aic, RMSE_test_aic)
#beta
ggplot(data = rmse_df_aic[1], aes(x = rmse_df_aic[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+
  geom_vline(xintercept = avg_rmse_beta, color = "red")
#mu
ggplot(data = rmse_df_aic[2], aes(x = rmse_df_aic[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+
  geom_vline(xintercept = avg_rmse_mu, color = "darkblue")
#test
ggplot(data = rmse_df_aic[3], aes(x = rmse_df_aic[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+
  geom_vline(xintercept = avg_rmse_test, color = "darkblue")

```


4.  Take a look at the summaries from the estimates under the best AIC model from the simulation that is equal to your team number.  Create confidence intervals for the $\beta$'s and comment on whether they include zero or not or the true value.
```{r}

```
5.   Use BIC with either stepwise or all possible subsets to select a model and then use OLS to estimate the parameters under that model.  Use the estimates to compute the RMSE for a) estimating $\beta^{true}$, b) $\mu^{true}$, and c) predicting $Y^{test}$. Present  histograms of the RMSE, and show where the average RMSE falls on the plot.   Also report d) the number of times you select the true model using BIC out of the 100 simulations. 
```{r}
#create vector with the same length and same name of betatrue
beta_like = numeric(length(betatrue))
names(beta_like) = names(betatrue)

#initialize RMSE vector
RMSE_beta_aic  = rep(0,100)
RMSE_mu_aic  = rep(0,100)
RMSE_test_aic  = rep(0,100)

for(i in 1:100){
#stepwise selection with BIC
app_best = step(ls[[i]], k=log(50),trace=0)
#create a copy of beta_like
beta_e = beta_like
#fill estimated beta with 0
beta_e[names(app_best$coefficients)] = app_best$coefficients
## for beta
RMSE_beta_aic[i] = rmse(betatrue,beta_e)
## for mu
RMSE_mu_aic[i] = rmse(mu, app_best$fitted.values)
## for testing data
RMSE_test_aic[i] = rmse(Y,predict(app_best, data = df[51:75,]))
}

#  average RMSE for estimating beta
avg_rmse_beta = mean(RMSE_beta_aic)

#  average RMSE for estimating mu
avg_rmse_mu = mean(RMSE_mu_aic)

# average RMSE for testing data
avg_rmse_test = mean(RMSE_test_aic)

#store RSME's in data frame
rmse_df_aic = data.frame(RMSE_beta_aic, RMSE_mu_aic, RMSE_test_aic)
#beta
ggplot(data = rmse_df_aic[1], aes(x = rmse_df_aic[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+
  geom_vline(xintercept = avg_rmse_beta, color = "red")
#mu
ggplot(data = rmse_df_aic[2], aes(x = rmse_df_aic[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+
  geom_vline(xintercept = avg_rmse_mu, color = "darkblue")
#test
ggplot(data = rmse_df_aic[3], aes(x = rmse_df_aic[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+
  geom_vline(xintercept = avg_rmse_test, color = "darkblue")


```

6.  Take a look at the summaries from the estimates under the best BIC model from the simulation that is equal to your team number.  Create confidence intervals for the $\beta$'s and comment on whether they include zero or not or the true value.

7. Theory (work individually and then combine to add group solution, try to complete by Wednesday before class)
For the linear model, assume that the $X$ have been centered so that they all have mean 0.  For the linear model
$$Y \sim N(1_n \beta_0 + X \beta, I_n/\phi)
$$
using Zellner's $g$-prior for  $\beta$ with 
$$\beta \mid \beta_0, \phi \sim N(0, g (X^TX)^{-1}/\phi)
$$
and the improper independent Jeffrey's prior $$p(\beta_0, \phi) \propto 1/\phi$$
find the a) posterior distriubtion of $\beta \mid Y, g, \phi$, b) posterior distribution of $\mu_i = x^T_i \beta \mid Y, g, \phi$ and c) the posterior predictive distribution of $Y^{test} \mid Y, g, \phi$ as functions of the OLS/MLE summaries. _(you may use results in notes - just quote - or derive)_

8. What are the corresponding distributions in 7) unconditional on $\phi$?  (hint recall theorem from class)  Are $\beta_0$ and $\beta$ still independent?  Explain.

9. Let $\tau = 1/g$ and substitute that in the prior for $\beta$
$$\beta \mid \beta_0, \phi \sim N(0, (X^TX)^{-1}/(\tau \phi))
$$
If $\tau \sim G(1/2, n/2)$, show that the prior on $\beta$ is a Cauchy Distribution 
$$\beta \mid  \phi, \beta_0 \sim C(0,  (X^TX/n)^{-1}/\phi)$$
_(a Cauchy distribution is a Student t with 1 df - see notes for density)_


_To speed up running time for the next set of problems, do the calculations for 9-13 in one named code chunk. then use separate code chunks to provide the necessary solutions for the different parts.  Test code using one or two simulated data sets, before running on all simulated data sets.  Once you are satisfied set cache=TRUE for the code chunk._

10.  Using Bayesian variable selection under the $g$-prior with $g = n$ and a uniform prior distribution over models,  find the highest posterior probability model (HPM)  using `bas.lm` from library `BAS` (or other software). (If you use `BAS`, please download `BAS` version 1.4.3 from CRAN).  Using the mean of the appropriate posterior distribution under the HPM, find the average RMSE for a) estimating $\beta^{true}$, b) estimating $\mu^{true}$ and c) predicting $Y^{test}$.  Plot histograms of the RMSE and add the average RMSE to the plots.   What proportion of the time did you select the true model?   Your answer should describe whether you used enumeration or MCMC, number of iterations or models, etc.  If you used MCMC, check diagnostic plots to examin convergence.  
Note `BAS` has functions to compute the fitted values `fitted` and predicted values `predict` for the HPM (see the vignette or help files), however, to find the posterior mean for the beta's for a given model, we need to extract the information from the object.  The following function can be use to do this. 

```{r}
coef.HPM = function(object) {
  best = which.max(object$postprobs)
  model = object$which[[best]]
  post.mean = object$mle[[best]]*object$shrinkage[best]
  return(list(HPM = model, betahat = post.mean))
}

```

 (Look at before Wednesday to be prepared to ask any questions)

11.  Using the simulation that is equal to your team numbers, provide posterior summaries of the coefficient's of the HPM, such as Bayesian Confidence intervals.  
Comment on whether the intervals contain the true value or zero.

12.  To incorporate model uncertainty we could use  Bayesian Model Averaging, rather than the highest probability model. Repeat 10 and 11 using BMA for estimating the quantities.  


13.  If we wanted to select the model that is "closest" to BMA,  we could use the model whose predictions are closest to BMA  using squared error loss.  We can find the best predictive model `BPM` from `BAS` using the predict function with `estimator="BPM"`.   Repeat 10 and 11 using the Best Predictive Model, `BPM`.


14.  Are the Bayesian estimates sensitive to the choice of prior?  Try 10-13 using the Zellner-Siow Cauchy prior (option `prior = "ZS-null"` in `bas.lm`)  


15.  Provide a summary of your simulation findings, with a table for the RMSEs for the different methods and parameters of interest $\beta$, $\mu$, $Y^{test}$ and proportion of time true model was selected.
Does any one method seem to do better than the others or are some methods better for one estimation/prediction problem than the others?  Explain.
For the energetic team - what about coverage?







