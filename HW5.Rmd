---
pdf_document: default
author: "Thomas Fleming, Blaire Li, Marc D. Ryser, Hengqian Zhang"
date: "Due March 10, 2016"
output: pdf_document
title: 'HW5: Team 12'
---


```{r setup, warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE)
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(GGally))
library(BAS)
library(knitr)
# post on piazza for additional packages if there are wercker build errors due to missing packages
```


```{r}
## From http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


For this assignment we will explore simulation of data to compare methods for estimation and model selection.  To get started, refer to the code from Lab6 and simulate the datasets as described there.  Some "guideposts" for when to finish parts are provided within the problem set.

1.  Add to the Lab6 code a second set of 100 datasets for testing (prediction) with $25$ observations, but where the $X$'s have the same correlation matrix as in the training data.   Provide a brief description of the model that generated the data and summary of the simulation study.  (ie dimensions, true $\beta$ etc, number of simulated datasets etc.).


ANSWER:
The true betas' we choose is the same as the betas' in lab6 which are {4,2,0,0,0,-1,0,1.5,0,0,0,1,0,0.5,0,0,0,0,-1,1,4}. We simulated 100 datasets and each with dimension 25 by 21. The procedure we use to generate the data is exactly the same as we did in lab 6. Considering the training data set, we wanted to simulate 100 datasets each with dimension  75 by 23 (21 for beta plus Y and mu)
For each dataset, first we generated 75 by 10 from standard normal distribution.

Secondly, we picked up the last 5 columns of the data we generated in step 1 and did matrix multiplication with {0.3,0.5,0.7,0.9,1.1} to get 75$\times$ 1 vector and  replicated it 5 times to get X1 with dimension 75 $\times$ 5

Thirdly, we generated 75 $\times$ 4 matrix from standard normal distribution and picked up the 4th column and added error from normal(0,0.1) to get X2 and X3 respectively.

Finally, we used cbind to combine X1, X2, X3 to get desired dataset and also computed Y and mu.

In addition, we can see the similar pattern in both correlation heatmap of training data and testing data. In simulation, there are 5 variables(columns) have correlation to each other. In heat map, there are ranctangles around the diagnal indicating the same patterns.
```{r Prob1, echo=FALSE}
set.seed(8675309)
# true parameters
sigma = 2.5
betatrue = c(4,2,0,0,0,-1,0,1.5, 0,0,0,1,0,.5,0,0,0,0,-1,1,4)
#          int|    X1                            | X2     |X3 

names(betatrue) = c("(Intercept)","x1","x2","x3","x4","x5",
                    "x6","x7","x8","x9","x10",
                    "x11","x12","x13","x14","x15",
                    "x16","x17","x18","x19","x20")

#sample size
n = 75

# part of dataframe name
fname=rep("df",100)

# create 100 datasets
for (i in 1:100) {
  
  # generate some standard normals
  Z = matrix(rnorm(n*10, 0, 1), ncol=10, nrow=n)
  
  #  Create X1 by taking linear cominations of Z to induce correlation among X1 components
  #multiplying first 5 columns by vector of numbers to create linear dependence
  X1 = cbind(Z, (Z[,1:5] %*% c(.3, .5, .7, .9, 1.1) %*% t(rep(1,5)) +
                   matrix(rnorm(n*5, 0, 1), ncol=5, nrow=n)))
  # generate X2 as a standard normal  
  #X2 independent, but correlated with X3
  X2 <- matrix(rnorm(n*4,0,1), ncol=4, nrow=n)
  
  # Generate X3 as a linear combination of X2 and noise  
  X3 <- X2[,4]+rnorm(n,0,sd=0.1)
  
  # combine them  
  X <- cbind(X1,X2,X3)
  
  # subtract off the column means
  X = sweep(X, 2, apply(X,2, mean)) 
  
  # Generate mu     
  # X does not have a column of ones for the intercept so need to add the intercept  
  # for true mu  
  mu = betatrue[1] + X %*% betatrue[-1] 
  
  # now generate Y  
  Y = mu + rnorm(n,0,sigma)  
  
  # make a dataframe and save it
  df = data.frame(Y, X, mu)
  colnames(df) = c("Y","x1","x2","x3","x4","x5",
                   "x6","x7","x8","x9","x10",
                   "x11","x12","x13","x14","x15",
                   "x16","x17","x18","x19","x20",
                   "mu")
  fname[i] = paste("df", as.character(i), sep="")
  save(df, file=fname[i])
}

load(fname[1])

par(mfrow = c(1,2))
#correlation map for training data 
image(cor(df[1:50,2:22]))
#correlation map for testing data 
image(cor(df[51:75,2:22]))
```


2.  Using Ordinary Least squares based on fitting the full model for each of the 100 data sets,  Compute the average RMSE for a) estimating $\beta^{true}$, b) estimating
$\mu^{true} = X \beta^{true}$ and c) out of sample prediction for the test data from the 100 data sets. Present histograms of the RMSEs and show where the average falls.
Note for a vector of length $d$, RMSE is defined as
$$
RMSE(\hat{\theta}) = \sqrt{\sum_{i = 1}^{d} (\hat{\theta}_j - \theta_j)^2/d}
$$


```{r, Prob2, echo=FALSE}

RMSE.beta  = rep(0,100)
RMSE.mu  = rep(0,100)
RMSE.test.pred  = rep(0,100)

ls = vector("list",100)

#Load simulation data frames and compute RMSE's
for( i in 1:100) {
  rm(df)
  load(fname[i])
  ls[[i]] = lm(Y ~ . -mu, data=df[1:50,])
  beta.ols = coef(ls[[i]])
  X.ols = model.matrix(Y ~ . -mu, data=df[1:50,])
  RMSE.beta[i] = sqrt(mean((betatrue - beta.ols)^2))
  RMSE.mu[i] = sqrt(mean((mu[1:50,] - X.ols %*% beta.ols)^2))
  RMSE.test.pred[i] = sqrt(mean((Y[51:75] - predict(ls[[i]], newdata = df[51:75,]))^2))
}

#  average RMSE for estimating beta
avg.rmse.beta = mean(RMSE.beta)

#  average RMSE for estimating mu
avg.rmse.mu = mean(RMSE.mu)

# average RMSE for test set predictions
avg.rmse.test = mean(RMSE.test.pred)

#store average RMSE's in data frame
avg.rmse.df = data.frame(beta = avg.rmse.beta, mu = avg.rmse.mu, test = avg.rmse.test)

#Table for average RMSE's
kable(avg.rmse.df, caption = "Average RMSE's")

#store RSME's in data frame
rmse.df = data.frame(RMSE.beta, RMSE.mu, RMSE.test.pred)


#histogram: beta
p1<-ggplot(data = rmse.df[1], aes(x = rmse.df[1])) + theme_light() + labs(x = "RSME: Beta", y = "Count") + geom_histogram(fill = "lightblue", color = "darkblue",bins=30) + geom_vline(xintercept = avg.rmse.beta, color = "red")+labs(title="AIC Step Selection")

#histogram: mu
p2<-ggplot(data = rmse.df[2], aes(x = rmse.df[2])) + theme_light() + labs(x = "RMSE: Mu", y = "Count") + geom_histogram(fill = "khaki", color = "darkgoldenrod", bins=20) + geom_vline(xintercept = avg.rmse.mu, color = "darkblue")

#histogram: test prediction
p3<-ggplot(data = rmse.df[3], aes(x = rmse.df[3])) + theme_light() + labs(x = "RMSE: Test Predictions", y = "Count") + geom_histogram(fill = "salmon", color = "orangered3", bins=20) + geom_vline(xintercept = avg.rmse.test, color = "darkblue")

multiplot(p1, p2, p3, cols=2)


```
The average RMSE for $\beta^{true}$, $\mu^{true}$ and out of sample prediction are `r avg.rmse.df[1]`, `r avg.rmse.df[2]`, and `r avg.rmse.df[3]`, respectively.

3.  Use AIC with either stepwise or all possible subsets to select a model and then use OLS to estimate the parameters under that model.  Using the estimates to compute the RMSE for a) estimating $\beta^{true}$, b) estimating $\mu^{true}$, and c) predicting $Y^{test}$. Present  histograms of the RMSE, and show where the  average RMSE falls on the plot.   Also report d) the number of times you select the true model using AIC out of the 100 simulations. 

```{r, Prob3, echo=FALSE, cache=TRUE}

#rmse function
rmse = function(y, ypred){
  rmse=sqrt(mean((y-ypred)^2))
  return(rmse)
}

#create vector with the same length and same name of betatrue
beta_like = numeric(length(betatrue))
names(beta_like) = names(betatrue)

beta_model = betatrue
beta_model[which(beta_model!=0)] = 1

#initialize RMSE vector
RMSE_beta_aic  = rep(0,100)
RMSE_mu_aic  = rep(0,100)
RMSE_test_aic  = rep(0,100)

count_true = 0
for(i in 1:100){
  rm(df)
  load(fname[i])
  #stepwise selection with AIC
  app_best = step(ls[[i]], k=2,trace=0)
  #create a copy of beta_like
  beta_e = beta_like
  #fill estimated beta with 0
  beta_e[names(app_best$coefficients)] = app_best$coefficients
  
  coe_position = beta_like
  coe_position[names(app_best$coefficients)] = 1
  if(all(coe_position == beta_model)){
    count_true = count_true+1
  }
  
  ## for beta
  RMSE_beta_aic[i] = rmse(betatrue,beta_e)
  ## for mu
  RMSE_mu_aic[i] = rmse(mu[1:50], app_best$fitted.values)
  ## for testing data
  RMSE_test_aic[i] = rmse(Y[51:75],predict(app_best, newdata = df[51:75,]))
}

#  average RMSE for estimating beta
avg_rmse_beta = mean(RMSE_beta_aic)

#  average RMSE for estimating mu
avg_rmse_mu = mean(RMSE_mu_aic)

# average RMSE for testing data
avg_rmse_test = mean(RMSE_test_aic)

avg_rmse_aic<-data.frame(avg_rmse_beta,avg_rmse_mu,avg_rmse_test )

#store RSME's in data frame
rmse_df_aic = data.frame(RMSE_beta_aic, RMSE_mu_aic, RMSE_test_aic)
#beta
p1<-ggplot(data = rmse_df_aic[1], aes(x = rmse_df_aic[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+ labs(x = "RSME: Beta", y = "Count") +
  geom_vline(xintercept = avg_rmse_beta, color = "red")
#mu
p2<-ggplot(data = rmse_df_aic[2], aes(x = rmse_df_aic[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+ labs(x = "RSME: mu", y = "Count") +
  geom_vline(xintercept = avg_rmse_mu, color = "darkblue")
#test
p3<-ggplot(data = rmse_df_aic[3], aes(x = rmse_df_aic[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+ labs(x = "RSME: Test Prediction", y = "Count") +
  geom_vline(xintercept = avg_rmse_test, color = "darkblue")

multiplot(p1, p2, p3, cols=2)


```
The number of "correct" models is `count_true`. The average RMSE for $\beta^{true}$, $\mu^{true}$ and out of sample prediction are `r avg_rmse_aic[1]`, `r avg_rmse_aic[2]`, and `r avg_rmse_aic[3]`, respectively.


4.  Take a look at the summaries from the estimates under the best AIC model from the simulation that is equal to your team number.  Create confidence intervals for the $\beta$'s and comment on whether they include zero or not or the true value.
```{r, Prob4, echo=FALSE}
app_best_12 = step(ls[[12]], k=2,trace=0)
beta_like = numeric(length(betatrue))
names(beta_like) = names(betatrue)
beta_like[names(coef(app_best_12))]<-coef(app_best_12)
beta_like<-rbind(beta_like, rep(0,length(beta_like)))
beta_like[2,names(coef(app_best_12))]<-confint(app_best_12)[,1]
beta_like<-rbind(beta_like, rep(0,length(beta_like)))
beta_like[3,names(coef(app_best_12))]<-confint(app_best_12)[,2]

MCT<-rbind(beta_like,betatrue) 
rownames(MCT)<-NULL
colnames(MCT)<-NULL
MCT<-as.data.frame(t(MCT))
colnames(MCT)<-c("mean", "lci", "uci","truth")
ggplot(MCT, aes(x=names(betatrue), y=mean),show.legend = TRUE) + 
    geom_errorbar(aes(ymin=lci, ymax=uci), width=.1) +
    geom_point()+geom_point(aes(y=truth),color="red")+labs(title="Stepwise AIC Model Selection (black=estimate, red=truth)", x="Coefficients", y="Value")

```

From the figure we see that with the exception of coefficients 9, 13, 18 and 19, the confidence intervals include the true value of $\beta$. Coefficients 9 and 17 are nonzero although the true values are zero. Conversely, coefficeints 18 and 19 are not included in the best model although their true values are non-zero.


5.   Use BIC with either stepwise or all possible subsets to select a model and then use OLS to estimate the parameters under that model.  Use the estimates to compute the RMSE for a) estimating $\beta^{true}$, b) $\mu^{true}$, and c) predicting $Y^{test}$. Present  histograms of the RMSE, and show where the average RMSE falls on the plot.   Also report d) the number of times you select the true model using BIC out of the 100 simulations. 
```{r, Prob5, echo=FALSE, cache=TRUE}
#create vector with the same length and same name of betatrue
beta_like = numeric(length(betatrue))
names(beta_like) = names(betatrue)


beta_model = betatrue
beta_model[which(beta_model!=0)] = 1

#initialize RMSE vector
RMSE_beta_bic  = rep(0,100)
RMSE_mu_bic  = rep(0,100)
RMSE_test_bic  = rep(0,100)

for(i in 1:100){
  rm(df)
  load(fname[i])
  #stepwise selection with BIC
  app_best = step(ls[[i]], k=log(50),trace=0)
  #create a copy of beta_like
  beta_e = beta_like
  #fill estimated beta with 0
  beta_e[names(app_best$coefficients)] = app_best$coefficients
  
  coe_position = beta_like
  coe_position[names(app_best$coefficients)] = 1
  if(all(coe_position == beta_model)){
    count_true = count_true+1
  }
  
  
  ## for beta
  RMSE_beta_bic[i] = rmse(betatrue,beta_e)
  ## for mu
  RMSE_mu_bic[i] = rmse(mu[1:50], app_best$fitted.values)
  ## for testing data
  RMSE_test_bic[i] = rmse(Y[51:75],predict(app_best, newdata = df[51:75,]))
}

#  average RMSE for estimating beta
avg_rmse_beta = mean(RMSE_beta_bic)

#  average RMSE for estimating mu
avg_rmse_mu = mean(RMSE_mu_bic)

# average RMSE for testing data
avg_rmse_test = mean(RMSE_test_bic)

avg_rmse_bic<-data.frame(avg_rmse_beta,avg_rmse_mu,avg_rmse_test)

#store RSME's in data frame
rmse_df_bic = data.frame(RMSE_beta_bic, RMSE_mu_bic, RMSE_test_bic)
#beta
p1<-ggplot(data = rmse_df_bic[1], aes(x = rmse_df_bic[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+
  geom_vline(xintercept = avg_rmse_beta, color = "red")
#mu
p2<-ggplot(data = rmse_df_bic[2], aes(x = rmse_df_bic[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+
  geom_vline(xintercept = avg_rmse_mu, color = "darkblue")
#test
p3<-ggplot(data = rmse_df_bic[3], aes(x = rmse_df_bic[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+
  geom_vline(xintercept = avg_rmse_test, color = "darkblue")

multiplot(p1,p2,p3, cols = 2)

```
The number of "correct" models is `count_true`. The average RMSE for $\beta^{true}$, $\mu^{true}$ and out of sample prediction are `r avg_rmse_bic[1]`, `r avg_rmse_bic[2]`, and `r avg_rmse_bic[3]`, respectively.


6.  Take a look at the summaries from the estimates under the best BIC model from the simulation that is equal to your team number.  Create confidence intervals for the $\beta$'s and comment on whether they include zero or not or the true value.
```{r, Prob6, echo=FALSE}
app_best_12 = step(ls[[12]], k=log(50),trace=0)
beta_like = numeric(length(betatrue))
names(beta_like) = names(betatrue)
beta_like[names(coef(app_best_12))]<-coef(app_best_12)
beta_like<-rbind(beta_like, rep(0,length(beta_like)))
beta_like[2,names(coef(app_best_12))]<-confint(app_best_12)[,1]
beta_like<-rbind(beta_like, rep(0,length(beta_like)))
beta_like[3,names(coef(app_best_12))]<-confint(app_best_12)[,2]

MCT<-rbind(beta_like,betatrue) 
rownames(MCT)<-NULL
colnames(MCT)<-NULL
MCT<-as.data.frame(t(MCT))
colnames(MCT)<-c("mean", "lci", "uci","truth")
ggplot(MCT, aes(x=names(betatrue), y=mean),show.legend = TRUE) + 
    geom_errorbar(aes(ymin=lci, ymax=uci), width=.1) +
    geom_point()+geom_point(aes(y=truth),color="red")+labs(title="Stepwise BIC Model Selection (black=estimate, red=truth)", x="Coefficients", y="Value")

```

From the figure we see that with the exception of coefficients 9, 13, 18 and 19, and 20, the confidence intervals include the true value of $\beta$. Coefficient 9 is nonzero although the true value is zero. Conversely, coefficeints 5, 18 and 19 are not included in the best model although their true values are non-zero.



7. Theory (work individually and then combine to add group solution, try to complete by Wednesday before class)
For the linear model, assume that the $X$ have been centered so that they all have mean 0.  For the linear model
$$Y \sim N(1_n \beta_0 + X \beta, I_n/\phi)
$$
using Zellner's $g$-prior for  $\beta$ with 
$$\beta \mid \beta_0, \phi \sim N(0, g (X^TX)^{-1}/\phi)
$$
and the improper independent Jeffrey's prior $$p(\beta_0, \phi) \propto 1/\phi$$
find the a) posterior distriubtion of $\beta \mid Y, g, \phi$, b) posterior distribution of $\mu_i = x^T_i \beta \mid Y, g, \phi$ and c) the posterior predictive distribution of $Y^{test} \mid Y, g, \phi$ as functions of the OLS/MLE summaries. _(you may use results in notes - just quote - or derive)_

ANSWER: a)
$$ p(\beta | Y, g, \phi) \propto p(Y | \beta, g, \phi) p(\beta | g, \phi) $$

Since Y is independent of g and Zellner's g-prior for $\beta$ is actually independent of $\beta_{0}$, we can plug in the given distributions for Y and $\beta$:

$= N(1_{n}\beta_{0} + X\beta, I_{n}/\phi,) N(0, g(X^tX)^{-1}/\phi)$

$$\propto e^{-((y - (1_{n}\beta_{0} + X\beta))^T (I_{n}/\phi)^{-1} (y - (1_{n}\beta_{0} + X\beta)))/2 } e^{- \beta^t (g(X^TX)^{-1}/\phi)^{-1} \beta/2} $$

$$\propto e^{- \dfrac{1}{2\sigma^2}((y - (1_{n}\beta_{0} + X\beta))^T(y - (1_{n}\beta_{0} + X\beta))) } e^{- \dfrac{1}{2 \sigma^2} \beta^t(X^TX)/g \beta} $$

$$ \propto e^{- \dfrac{1}{2 \sigma^2} (y^T y - 2\beta^TX^Ty - \beta^TX^TX\beta - 2_n\beta_{0}^Ty + 1_{n}\beta_{0}1_{n}\beta_{0} + \beta^T((X^TX)/g)\beta) } = \beta^T A \beta - 2\beta B + C $$

where $$ A = \dfrac{(1 + g) X^T X}{g \sigma^2} $$, $$ B = \dfrac{X^T y}{\sigma^2} $$, and $$ c = c(y, \beta_{0}) $$

Since c is not a function of $\beta$, we can leave it out of the following proportion and rewrite the above in terms of A and B as,

$$ e^{- \dfrac{1}{2} (\beta^T A \beta - 2 \beta B)} $$

While completing the square in this situation is tedious, we can use knowledge of the strategy for the univariate normal, outlined on page 70 of Hoff, where it is easier to calculate the posterior mean and variance. It can be shown that the formulas for the posterior mean and variance also apply to the multivariate case, and so we know that the posterior mean is equal to $$ \dfrac{A}{B} $$ and the posterior variance is equal to $$ \dfrac{1}{A} $$. If we expand these equations out, using the right-hand side of equations A and B, and simplify, we find that we have a normal distribution with:


$$Var[\beta | Y, g, \phi] = \dfrac{g}{g + 1}\sigma^2(X^tX)^{-1}$$

$$E[\beta | Y, g, \phi] = \dfrac{g}{g + 1}(X^TX)^{-1}X^Ty $$



b) Since the model matrix does not vary, we can find the distribution for $$ \mu_{i} = x_{i}^t\beta | Y, g, \phi $$ by taking the expected value and variance of $$ \beta | Y, g, \phi $$:

$$ E[x_i\beta | Y, g, \phi] = x_iE[ \beta | Y, g, \phi]$$

$$ = x_i\dfrac{g}{g + 1}(X^TX)^{-1}X^Ty $$

$$ Var[x_i\beta | Y, g, \phi] = x_i^t x_i Var[\beta | Y, g, \phi]$$

$$ = x_i^t x_i \dfrac{g}{g + 1} \sigma^2(X^TX)^{-1} $$

*In the derivations below, the $\beta_{0}$, $\beta$, $\epsilon$, $X$, and $\sigma$ represent those associated with the test set. While we could have taken pains to label each of them "test", we have left this notation out for the sake of simplicity.

c) $$ E[Y^{test} | Y, g, \phi] = E[ 1_n \beta_{0} + X\beta + \epsilon^ | Y, g, \phi]$$

$$ = E[ 1_n \beta_{0} | Y, g, \phi] + E[X\beta | Y, g, \phi] + E[\epsilon^ | Y, g, \phi] $$

$$ = \dfrac{1}{\phi} + x_i \dfrac{g}{1 + g} (X^TX)^{-1}X^Ty + 0$$

$$ = \dfrac{1}{\phi} + x_i \dfrac{g}{1 + g} (X^TX)^{-1}X^Ty $$

$$Var[Y^{test} | Y, g, \phi] =  Var[ 1_n \beta_{0} + X\beta + \epsilon^ | Y, g, \phi] $$

$$ = Var[1_n \beta_{0} | Y, g, \phi] + Var[X \beta | Y, g, \phi] + Var[\epsilon^ | Y, g, \phi] $$

$$ = \dfrac{1}{n \phi} + \dfrac{g}{g + 1}\sigma^2(X^TX)^{-1} + \dfrac{I_n}{\phi}$$

$$ = \dfrac{1}{\phi} ( \dfrac{1}{n} + \dfrac{g}{g + 1}(X^TX)^{-1} + I_n) $$


8. What are the corresponding distributions in 7) unconditional on $\phi$?  (hint recall theorem from class)  Are $\beta_0$ and $\beta$ still independent?  Explain.

ANSWER:

a)

$$ p(\beta | Y, g) = \int p(\beta | Y, g, \phi) p(\phi | Y, g) d\phi $$
$$ = \int (2\pi)^{-p/2} \begin{vmatrix}\dfrac{g}{(g + 1)\phi}(X^TX)^{-1}\end{vmatrix}^{-1/2}  exp(-\dfrac{(\beta - \dfrac{g}{g + 1}\hat{\beta}_{OLS})^T (\dfrac{g}{g + 1}\sigma^2(X^TX)^{-1})^{-1} (\beta - \dfrac{g}{g + 1}\hat{\beta}_{OLS})}{2}) \dfrac{(\dfrac{\nu_{n}\hat{\sigma^2}}{2})}{\Gamma(\nu_{n}/2)}^{\nu_{n}/2} \phi^{\nu_{n}/2 - 1} exp(-\dfrac{\nu_{n}\hat{\sigma^2}}{2}) d\phi $$

$$ \propto \int \phi^{p/2} \phi^{\nu_{n}/2 -1} exp(-\dfrac{(\beta - \dfrac{g}{g + 1}\hat{\beta}_{OLS})^T (\dfrac{g + 1}{g }\phi(X^TX)) (\beta - \dfrac{g}{g + 1}\hat{\beta}_{OLS})}{2}) exp(-\dfrac{\nu_{n}\hat{\sigma^2}}{2}) d\phi $$

$$ \propto \int \phi^{(p+\nu_{n})/2 -1} exp(-\dfrac{(\beta - \dfrac{g}{g + 1}\hat{\beta}_{OLS})^T (\beta - \dfrac{g}{g + 1}\hat{\beta}_{OLS})}{2}) + \nu_{n}\hat{\sigma^2}}{2}\phi) d\phi $$ 

This is the kernel of a gamma and we know that it is equal to the reciprocal of its proportional constant:

$$  = \Gamma(\dfrac{p + \nu_{n}}{2}) \Big[\dfrac{(Y - (1_{n}\beta_{0} + X\beta))^T (Y - (1_{n}\beta_{0} + X\beta)) + \nu_{n}\hat{\sigma^2}}{2}\Big]^{-(p + \nu_{n})/2} \propto \Big[(Y - (1_{n}\beta_{0} + X\beta))^T (Y - (1_{n}\beta_{0} + X\beta)) + \nu_{n}\hat{\sigma^2}\Big]^{-(p + \nu_{n})/2} $$

$$ \propto \Big[1 + \dfrac{1}{\nu_{n}}\dfrac{(Y - (1_{n}\beta_{0} + X\beta))^T (Y - (1_{n}\beta_{0} + X\beta))}{\hat{\sigma^2}}\Big]^{-(p+\nu_{n})/2} $$

This is proportional to a Student-T distribution with $\nu_{n}$ degrees of freedom.

b)

$$ p(x_{i}^T\beta | Y, g) = \int p(x_{i}^T\beta | Y, g, \phi) p(\phi | Y, g) d\phi $$

$$ = \int (2\pi)^{-p/2} \begin{vmatrix}\dfrac{I_{n}}{\phi}\end{vmatrix}^{-1/2} exp(-\dfrac{(Y - (1_{n}\beta_{0} + X\beta))^T (I_{n}/\phi)^{-1} (Y - (1_{n}\beta_{0} + X\beta))}{2}) \dfrac{(\dfrac{\nu_{n}\hat{\sigma^2}}{2})}{\Gamma(\nu_{n}/2)}^{\nu_{n}/2} \phi^{\nu_{n}/2 - 1} exp(-\dfrac{\nu_{n}\hat{\sigma^2}}{2}) d\phi $$

$$ \propto \int \phi^{p/2} \phi^{\nu_{n}/2 -1} exp(-\dfrac{(Y - (1_{n}\beta_{0} + X\beta))^T (Y - (1_{n}\beta_{0} + X\beta))}{2}\phi) exp(-\dfrac{\nu_{n}\hat{\sigma^2}}{2}) d\phi $$

$$ \propto \int \phi^{(p+\nu_{n})/2 -1} exp(-\dfrac{(Y - (1_{n}\beta_{0} + X\beta))^T (Y - (1_{n}\beta_{0} + X\beta)) + \nu_{n}\hat{\sigma^2}}{2}\phi) d\phi $$ 

This is the kernel of a gamma and we know that it is equal to the reciprocal of its proportional constant:

$$  = \Gamma(\dfrac{p + \nu_{n}}{2}) \Big[\dfrac{(Y - (1_{n}\beta_{0} + X\beta))^T (Y - (1_{n}\beta_{0} + X\beta)) + \nu_{n}\hat{\sigma^2}}{2}\Big]^{-(p + \nu_{n})/2} \propto \Big[(Y - (1_{n}\beta_{0} + X\beta))^T (Y - (1_{n}\beta_{0} + X\beta)) + \nu_{n}\hat{\sigma^2}\Big]^{-(p + \nu_{n})/2} $$

$$ \propto \Big[1 + \dfrac{1}{\nu_{n}}\dfrac{(Y - (1_{n}\beta_{0} + X\beta))^T (Y - (1_{n}\beta_{0} + X\beta))}{\hat{\sigma^2}}\Big]^{-(p+\nu_{n})/2} $$

9. Let $\tau = 1/g$ and substitute that in the prior for $\beta$
$$\beta \mid \beta_0, \phi \sim N(0, (X^TX)^{-1}/(\tau \phi))$$
If $\tau \sim G(1/2, n/2)$, show that the prior on $\beta$ is a Cauchy Distribution 
$$\beta \mid  \phi, \beta_0 \sim C(0,  (X^TX/n)^{-1}/\phi)$$
_(a Cauchy distribution is a Student t with 1 df - see notes for density)_

ANSWER:

$$p(\beta | \beta_{0}, \phi) = \int p(\beta | \beta_{0}, \phi, g)p(\tau | \phi, \beta_{0})p(\phi, \beta_{0}) d\tau $$

Since $\tau$ does not depend on $\phi$ or $\beta_{0}$, we can use the given unconditional distribution for $\tau$. Plugging in the distributions, we get:

$$ = \int (2\pi)^{-p/2} \begin{vmatrix}\dfrac{(X^TX)^{-1}}{\tau \phi}\end{vmatrix}^{-1/2} exp(-\dfrac{\beta^T(\dfrac{(X^TX)}{\tau\phi})^{-1}\beta}{2})  \dfrac{\dfrac{n}{2}^{1/2}}{\Gamma(\dfrac{1}{2})} \tau^{1/2 - 1} exp(-\dfrac{n}{2}\tau) \dfrac{1}{\phi} d\tau $$

Grouping terms, we get:

$$ \propto \int \tau^{(p + 1)/2 - 1} exp(-\dfrac{(n +(\beta^T\phi(X^TX)\beta))}{2} \tau) d\tau $$

It is easy to see that this is the kernel of a Gamma distribution with $a = \dfrac{n + 1}{2}$ and $b = \dfrac{n + (\beta^T\phi(X^TX)\beta)}{2} $ . For this expression to be equal to 1, the proportional constant would need to be $ \dfrac{(\dfrac{n + (\beta^T\phi(X^TX)\beta)}{2})^{(n+1)/2}}{\Gamma(\dfrac{n + 1}{2})} $. It can be shown that the reciprocal of this proportional constant is what the above proportion is equal to. In mathematical terms:

$$ = \Gamma(\dfrac{p + 1}{2}) \Big[(\dfrac{n +(\beta^T\phi(X^TX)\beta)}{2})^{-(p+1)/2}\Big] \propto (n +(\beta^T\phi(X^TX)\beta))^{-(p+1)/2} $$
$$ \propto \Big[1 + \dfrac{\beta^T\phi(X^TX)\beta}{n}\Big]^{-(p+1)/2}$$

So, the prior on $\beta$ is proportional to a student-T with one degree of freedom, which is also a Cauchy distribution.



10.  Using Bayesian variable selection under the $g$-prior with $g = n$ and a uniform prior distribution over models,  find the highest posterior probability model (HPM)  using `bas.lm` from library `BAS` (or other software). (If you use `BAS`, please download `BAS` version 1.4.3 from CRAN).  Using the mean of the appropriate posterior distribution under the HPM, find the average RMSE for a) estimating $\beta^{true}$, b) estimating $\mu^{true}$ and c) predicting $Y^{test}$.  Plot histograms of the RMSE and add the average RMSE to the plots.   What proportion of the time did you select the true model?   Your answer should describe whether you used enumeration or MCMC, number of iterations or models, etc.  If you used MCMC, check diagnostic plots to examin convergence.  
Note `BAS` has functions to compute the fitted values `fitted` and predicted values `predict` for the HPM (see the vignette or help files), however, to find the posterior mean for the beta's for a given model, we need to extract the information from the object.  The following function can be use to do this. 



```{r, Prob 10, echo=FALSE, cache=TRUE}
coef.HPM = function(object) {
  best = which.max(object$postprobs)
  model = object$which[[best]]
  post.mean = object$mle[[best]]*object$shrinkage[best]
  return(list(HPM = model, betahat = post.mean))
}

coef.BPM = function(object) {
  yy = predict(object, estimator="BPM")
  model = object$which[[yy$best]]
  post.mean = object$mle[[yy$best]]*object$shrinkage[yy$best]
  return(list(HPM = model, betahat = post.mean))
}


#################################
### Chunk for Problems 10 and 12
#################################

library(BAS)
#create vector with the same length and same name of betatrue
beta_like = numeric(length(betatrue))
names(beta_like) = names(betatrue)

count_true_hpm<-0
count_true_bpm<-0

beta_model[which(betatrue!=0)] = 1

#initialize RMSE vector for HPM
RMSE_beta_HPM  = rep(0,100)
RMSE_mu_HPM  = rep(0,100)
RMSE_test_HPM  = rep(0,100)

#initialize RMSE vector for BMA
RMSE_beta_BMA = rep(0,100)
RMSE_mu_BMA  = rep(0,100)
RMSE_test_BMA  = rep(0,100)

#initialize RMSE vector for BPM
RMSE_beta_BPM = rep(0,100)
RMSE_mu_BPM  = rep(0,100)
RMSE_test_BPM  = rep(0,100)

for(i in 1:100){
  rm(df)
  load(fname[i])
  # Use BAS for model sampling (method=MCMC for speed; method=BAS for enumeration)
  app_best = bas.lm(Y~.-mu, data=df[1:50,],
                    prior="g-prior", a=nrow(df[1:50,]), modelprior=uniform(),
                    method="MCMC", MCMC.iterations = 200000, thin = 20)
  
  ###############################################
  ## Work out the coefficients for the HPM ######
  # create a copy of beta_like
  beta_hpm = beta_like
  # call the function coef.HPM to compute the betahat's for the HPM
  hpm_summary<-coef.HPM(app_best)
  # create beta_hpm of length(betatrue) with zeros for left out predictors
  beta_hpm[hpm_summary$HPM+1] = hpm_summary$betahat
  
  # Check if the HPM is identical with the true model in terms of variable selection
  coe_position = (beta_hpm!=0)*1
  if(all(coe_position == beta_model)){
    count_true_hpm = count_true_hpm+1
  }
  
  ###############################################
  ## Work out the coefficients for the BPM ######
  # create a copy of beta_like
  beta_bpm = beta_like
  # call the function coef.HPM to compute the betahat's for the HPM
  bpm_summary<-coef.BPM(app_best)
  # create beta_hpm of length(betatrue) with zeros for left out predictors
  beta_bpm[bpm_summary$HPM+1] = bpm_summary$betahat
  
  # Check if the HPM is identical with the true model in terms of variable selection
  coe_position = (beta_bpm!=0)*1
  if(all(coe_position == beta_model)){
    count_true_bpm = count_true_bpm+1
  }
  
  ###############################################
  ## UPDATE THE RMSE VECTORS        ######
  
  ## HPM: RMSE for beta_true (fitting)
  RMSE_beta_HPM[i] = rmse(betatrue,beta_hpm)
  ## HPM: RMSE for mu_true (fitting)
  mu_fit<-fitted(app_best, estimator = "HPM")
  RMSE_mu_HPM[i] = rmse(mu[1:50], mu_fit)
  ## HPM: RMSE for Y_true (prediction on test data)
  y_pred<-predict.bas(app_best,newdata=df[51:75,], estimator="HPM", prediction=TRUE)
  RMSE_test_HPM[i] = rmse(Y[50:75],y_pred$fit)
  
  ## BMA: RMSE for beta_true (fitting)
  RMSE_beta_BMA[i] = rmse(betatrue,coef(app_best)$postmean)
  ## BMA: RMSE for mu_true (fitting)
  mu_fit<-fitted(app_best, estimator = "BMA")
  RMSE_mu_BMA[i] = rmse(mu[1:50], mu_fit)
  ## BMA: RMSE for Y_true (prediction on test data)
  y_pred<-predict.bas(app_best,newdata=df[51:75,], estimator="BMA", prediction=TRUE)
  RMSE_test_BMA[i] = rmse(Y[51:75],y_pred$fit)
  
  ## BPM: RMSE for beta_true (fitting)
  RMSE_beta_BPM[i] = rmse(betatrue,beta_bpm)
  ## BPM: RMSE for mu_true (fitting)
  mu_fit<-fitted(app_best, estimator = "BPM")
  RMSE_mu_BPM[i] = rmse(mu[1:50], mu_fit)
  ## BPM: RMSE for Y_true (prediction on test data)
  y_pred<-predict.bas(app_best,newdata=df[51:75,], estimator="BPM", prediction=TRUE)
  RMSE_test_BPM[i] = rmse(Y[51:75],y_pred$fit)
  
}


# Prepare for outputs

#store RSME's in data frame
RMSE_HPM = data.frame(RMSE_beta_HPM, RMSE_mu_HPM, RMSE_test_HPM)
RMSE_BMA = data.frame(RMSE_beta_BMA, RMSE_mu_BMA, RMSE_test_BMA)
RMSE_BPM = data.frame(RMSE_beta_BPM, RMSE_mu_BPM, RMSE_test_BPM)

#diagnostic plots for MCMC convergence
#diagnostics(app_best, type="pip")

```



```{r, 10_output, echo=FALSE, cache=TRUE}
RMSE<-RMSE_HPM

#beta
g1<-ggplot(data = RMSE[1], aes(x = RMSE[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+
  geom_vline(xintercept = mean(RMSE[,1]), color = "red") + labs(title="HPM: RMSE for beta ", x="RMSE", y="Count")
#mu
g2<-ggplot(data = RMSE[2], aes(x = RMSE[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+
  geom_vline(xintercept = mean(RMSE[,2]), color = "darkblue")+ labs(title="HPM: RMSE for mu ", x="RMSE", y="Count")
#test
g3<-ggplot(data = RMSE[3], aes(x = RMSE[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+
  geom_vline(xintercept = mean(RMSE[,3]), color = "darkblue")+ labs(title="HPM: RMSE for prediction", x="RMSE", y="Count")

multiplot(g1,g2,g3,cols = 2)

```


The number of times the "correct" model is selected is `r count_true_hpm`. 



11.  Using the simulation that is equal to your team numbers, provide posterior summaries of the coefficient's of the HPM, such as Bayesian Confidence intervals.  
Comment on whether the intervals contain the true value or zero.

```{r}


```


12.  To incorporate model uncertainty we could use  Bayesian Model Averaging, rather than the highest probability model. Repeat 10 and 11 using BMA for estimating the quantities.


```{r output12, echo=FALSE, cache=TRUE}
RMSE<-RMSE_BMA

#beta
b1<-ggplot(data = RMSE[1], aes(x = RMSE[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+
  geom_vline(xintercept = mean(RMSE[,1]), color = "red")+ labs(title="BMA: RMSE for beta ", x="RMSE", y="Count")
#mu
b2<-ggplot(data = RMSE[2], aes(x = RMSE[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+
  geom_vline(xintercept = mean(RMSE[,2]), color = "darkblue")+ labs(title="BMA: RMSE for mu ", x="RMSE", y="Count")
#test
b3<-ggplot(data = RMSE[3], aes(x = RMSE[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+
  geom_vline(xintercept = mean(RMSE[,3]), color = "darkblue")+ labs(title="BMA: RMSE for test prediction ", x="RMSE", y="Count")

multiplot(b1,b2,b3,cols = 2)

## confint


rm(df)
load(fname[12])
app_best_BMA12 = app_best = bas.lm(Y~.-mu, data=df[1:50,],
                    prior="g-prior", a=nrow(df[1:50,]), modelprior=uniform(),
                    method="MCMC", MCMC.iterations = 200000, thin = 20)


beta_like<-confint.coef.bas(coef(app_best_BMA12))


MCT<-as.data.frame(cbind(beta_like,betatrue) )
colnames(MCT)<-c( "lci", "uci","post.mean", "truth")
ggplot(MCT, aes(x=names(betatrue), y=post.mean),show.legend = TRUE) + 
    geom_errorbar(aes(ymin=lci, ymax=uci), width=.1) +
    geom_point()+geom_point(aes(y=truth),color="red")+labs(title="Bayesian Model Averaging (BMA) (black=estimate, red=truth)", x="Coefficients", y="Value")



```


From The figure we see that the true values of all coefficeint are contained in the posterior credible intervals. 19 and 20 are, once more, problematic in the sense that there is a very large credible interval. 


13.  If we wanted to select the model that is "closest" to BMA,  we could use the model whose predictions are closest to BMA  using squared error loss.  We can find the best predictive model `BPM` from `BAS` using the predict function with `estimator="BPM"`.   Repeat 10 and 11 using the Best Predictive Model, `BPM`.

```{r, 13_output, echo=FALSE, cache=TRUE}
RMSE<-RMSE_BPM

#beta
q1<-ggplot(data = RMSE[1], aes(x = RMSE[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+
  geom_vline(xintercept = mean(RMSE[,1]), color = "red")+ labs(title="BPM: RMSE for beta ", x="RMSE", y="Count")
#mu
q2<-ggplot(data = RMSE[2], aes(x = RMSE[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+
  geom_vline(xintercept = mean(RMSE[,2]), color = "darkblue")+ labs(title="BPM: RMSE for mu ", x="RMSE", y="Count")
#test
q3<-ggplot(data = RMSE[3], aes(x = RMSE[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+
  geom_vline(xintercept = mean(RMSE[,3]), color = "darkblue")+ labs(title="BPM: RMSE for test prediction ", x="RMSE", y="Count")

multiplot(q1,q2,q3,cols=2)

### Confidence interval ####

rm(df)
load(fname[12])
app_best_BMA12 = app_best = bas.lm(Y~.-mu, data=df[1:50,],
                    prior="g-prior", a=nrow(df[1:50,]), modelprior=uniform(),
                    method="MCMC", MCMC.iterations = 200000, thin = 20)

count_true_bpm
```



14.  Are the Bayesian estimates sensitive to the choice of prior?  Try 10-13 using the Zellner-Siow Cauchy prior (option `prior = "ZS-null"` in `bas.lm`)  

```{r, Prob14.1, echo=FALSE, cache=TRUE}

##############################
## Bulk code for 10-12
##############################
#create vector with the same length and same name of betatrue
beta_like = numeric(length(betatrue))
names(beta_like) = names(betatrue)

count_true_hpm<-0
count_true_bpm<-0

beta_model[which(betatrue!=0)] = 1

#initialize RMSE vector for HPM
RMSE_beta_HPM  = rep(0,100)
RMSE_mu_HPM  = rep(0,100)
RMSE_test_HPM  = rep(0,100)

#initialize RMSE vector for BMA
RMSE_beta_BMA = rep(0,100)
RMSE_mu_BMA  = rep(0,100)
RMSE_test_BMA  = rep(0,100)

#initialize RMSE vector for BPM
RMSE_beta_BPM = rep(0,100)
RMSE_mu_BPM  = rep(0,100)
RMSE_test_BPM  = rep(0,100)

for(i in 1:10){
  print(i)
  rm(df)
  load(fname[i])
  # Use BAS for model sampling (method=MCMC for speed; method=BAS for enumeration)
  bas.lm(Y ~.-mu, data=df[1:50,],
                    prior="ZS-null", a=nrow(df[1:50,]), modelprior=uniform(),
                    method="MCMC", MCMC.iterations = 200000, thin = 20,
                    initprobs="marg-eplogp")
  ###############################################
  ## Work out the coefficients for the HPM ######
  # create a copy of beta_like
  beta_hpm = beta_like
  # call the function coef.HPM to compute the betahat's for the HPM
  hpm_summary<-coef.HPM(app_best)
  # create beta_hpm of length(betatrue) with zeros for left out predictors
  beta_hpm[hpm_summary$HPM+1] = hpm_summary$betahat
  
  # Check if the HPM is identical with the true model in terms of variable selection
  coe_position = (beta_hpm!=0)*1
  if(all(coe_position == beta_model)){
    count_true_hpm = count_true_hpm+1
  }
  
  ###############################################
  ## Work out the coefficients for the BPM ######
  # create a copy of beta_like
  beta_bpm = beta_like
  # call the function coef.HPM to compute the betahat's for the HPM
  bpm_summary<-coef.BPM(app_best)
  # create beta_hpm of length(betatrue) with zeros for left out predictors
  beta_bpm[bpm_summary$HPM+1] = bpm_summary$betahat
  
  # Check if the HPM is identical with the true model in terms of variable selection
  coe_position = (beta_bpm!=0)*1
  if(all(coe_position == beta_model)){
    count_true_bpm = count_true_bpm+1
  }
  
  ###############################################
  ## UPDATE THE RMSE VECTORS        ######
  
  ## HPM: RMSE for beta_true (fitting)
  RMSE_beta_HPM[i] = rmse(betatrue,beta_hpm)
  ## HPM: RMSE for mu_true (fitting)
  mu_fit<-fitted(app_best, estimator = "HPM")
  RMSE_mu_HPM[i] = rmse(mu[1:50], mu_fit)
  ## HPM: RMSE for Y_true (prediction on test data)
  y_pred<-predict.bas(app_best,newdata=df[51:75,], estimator="HPM", prediction=TRUE)
  RMSE_test_HPM[i] = rmse(Y[50:75],y_pred$fit)
  
  ## BMA: RMSE for beta_true (fitting)
  RMSE_beta_BMA[i] = rmse(betatrue,coef(app_best)$postmean)
  ## BMA: RMSE for mu_true (fitting)
  mu_fit<-fitted(app_best, estimator = "BMA")
  RMSE_mu_BMA[i] = rmse(mu[1:50], mu_fit)
  ## BMA: RMSE for Y_true (prediction on test data)
  y_pred<-predict.bas(app_best,newdata=df[51:75,], estimator="BMA", prediction=TRUE)
  RMSE_test_BMA[i] = rmse(Y[51:75],y_pred$fit)
  
  ## BPM: RMSE for beta_true (fitting)
  RMSE_beta_BPM[i] = rmse(betatrue,beta_bpm)
  ## BPM: RMSE for mu_true (fitting)
  mu_fit<-fitted(app_best, estimator = "BPM")
  RMSE_mu_BPM[i] = rmse(mu[1:50], mu_fit)
  ## BPM: RMSE for Y_true (prediction on test data)
  y_pred<-predict.bas(app_best,newdata=df[51:75,], estimator="BPM", prediction=TRUE)
  RMSE_test_BPM[i] = rmse(Y[51:75],y_pred$fit)
  
}


# Prepare for outputs

#store RSME's in data frame
RMSE_HPM = data.frame(RMSE_beta_HPM, RMSE_mu_HPM, RMSE_test_HPM)
RMSE_BMA = data.frame(RMSE_beta_BMA, RMSE_mu_BMA, RMSE_test_BMA)
RMSE_BPM = data.frame(RMSE_beta_BPM, RMSE_mu_BPM, RMSE_test_BPM)
```

```{r, Prob14.2, echo=FALSE, cache=TRUE}
### OUTPUT FOR 10

RMSE<-RMSE_HPM

#beta
g1<-ggplot(data = RMSE[1], aes(x = RMSE[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+
  geom_vline(xintercept = mean(RMSE[,1]), color = "red") + labs(title="HPM: RMSE for beta ", x="RMSE", y="Count")
#mu
g2<-ggplot(data = RMSE[2], aes(x = RMSE[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+
  geom_vline(xintercept = mean(RMSE[,2]), color = "darkblue")+ labs(title="HPM: RMSE for mu ", x="RMSE", y="Count")
#test
g3<-ggplot(data = RMSE[3], aes(x = RMSE[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+
  geom_vline(xintercept = mean(RMSE[,3]), color = "darkblue")+ labs(title="HPM: RMSE for prediction", x="RMSE", y="Count")

multiplot(g1,g2,g3,cols = 2) 
```

```{r, Prob14.3, echo=FALSE}
####  FOR 11


```

```{r, Prob14.4, echo=FALSE, cache=TRUE}
# For Question 12
RMSE<-RMSE_BMA

#beta
b1<-ggplot(data = RMSE[1], aes(x = RMSE[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+
  geom_vline(xintercept = mean(RMSE[,1]), color = "red")+ labs(title="BMA: RMSE for beta ", x="RMSE", y="Count")
#mu
b2<-ggplot(data = RMSE[2], aes(x = RMSE[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+
  geom_vline(xintercept = mean(RMSE[,2]), color = "darkblue")+ labs(title="BMA: RMSE for mu ", x="RMSE", y="Count")
#test
b3<-ggplot(data = RMSE[3], aes(x = RMSE[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+
  geom_vline(xintercept = mean(RMSE[,3]), color = "darkblue")+ labs(title="BMA: RMSE for test prediction ", x="RMSE", y="Count")

multiplot(b1,b2,b3,cols = 2)

## confint


rm(df)
load(fname[12])
app_best_BMA12 =   bas.lm(Y ~.-mu, data=df[1:50,],
                    prior="ZS-null", a=nrow(df[1:50,]), modelprior=uniform(),
                    method="MCMC", MCMC.iterations = 200000, thin = 20,
                    initprobs="marg-eplogp")


beta_like<-confint.coef.bas(coef(app_best_BMA12))


MCT<-as.data.frame(cbind(beta_like,betatrue) )
colnames(MCT)<-c( "lci", "uci","post.mean", "truth")
ggplot(MCT, aes(x=names(betatrue), y=post.mean),show.legend = TRUE) + 
    geom_errorbar(aes(ymin=lci, ymax=uci), width=.1) +
    geom_point()+geom_point(aes(y=truth),color="red")+labs(title="Bayesian Model Averaging (BMA) (black=estimate, red=truth)", x="Coefficients", y="Value")



```


```{r, Prob14.5, echo=FALSE, cache=TRUE}

RMSE<-RMSE_BPM

#beta
q1<-ggplot(data = RMSE[1], aes(x = RMSE[1])) + 
  geom_histogram(fill = "lightblue", color = "darkblue")+
  geom_vline(xintercept = mean(RMSE[,1]), color = "red")+ labs(title="BPM: RMSE for beta ", x="RMSE", y="Count")
#mu
q2<-ggplot(data = RMSE[2], aes(x = RMSE[2])) + 
  geom_histogram(fill = "khaki", color = "darkgoldenrod")+
  geom_vline(xintercept = mean(RMSE[,2]), color = "darkblue")+ labs(title="BPM: RMSE for mu ", x="RMSE", y="Count")
#test
q3<-ggplot(data = RMSE[3], aes(x = RMSE[3])) + 
  geom_histogram(fill = "salmon", color = "orangered3")+
  geom_vline(xintercept = mean(RMSE[,3]), color = "darkblue")+ labs(title="BPM: RMSE for test prediction ", x="RMSE", y="Count")

multiplot(q1,q2,q3,cols=2)

### Confidence interval ####

rm(df)
load(fname[12])
app_best_BMA12 =   bas.lm(Y ~.-mu, data=df[1:50,],
                    prior="ZS-null", a=nrow(df[1:50,]), modelprior=uniform(),
                    method="MCMC", MCMC.iterations = 200000, thin = 20,
                    initprobs="marg-eplogp")
###????$$$
```

15.  Provide a summary of your simulation findings, with a table for the RMSEs for the different methods and parameters of interest $\beta$, $\mu$, $Y^{test}$ and proportion of time true model was selected.
Does any one method seem to do better than the others or are some methods better for one estimation/prediction problem than the others?  Explain.
For the energetic team - what about coverage?







